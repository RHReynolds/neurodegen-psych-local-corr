---
title: "GWAS preprocessing"
author: 
- name: "Aaron Wagen & Regina H. Reynolds"
  affiliation: UCL
output: 
  html_document:
    code_folding: show
    theme: paper
    highlight: kate
    df_print: paged
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include = F, message = F}

library(colochelpR) # Helper package with wrapper functions. See: https://github.com/RHReynolds/colochelpR.
library(data.table) # Loaded for fread(), which permits fast loading of files with many rows
library(here) # For file path construction
library(tidyverse) # For tidy manipulation of data
library(SNPlocs.Hsapiens.dbSNP144.GRCh37) # Used to add rs ids to GRCh37 genomic locations
library(stringr) # For string manipulation
library(rutils) # Own package with common R utility functions, including liftover of genomic coords. See: https://github.com/RHReynolds/rutils.

# Load dbsnp
dbsnp_144 <- SNPlocs.Hsapiens.dbSNP144.GRCh37

# Common file paths and extensions
gwas_dir <- "/data/LDScore/GWAS/"
lava_ext <- ".lava.gz"

# Load PD w/o UKBB
PD <- 
  fread(
    stringr::str_c(
      gwas_dir,
      "PD2019_meta5_ex23andMe_exUKBB/PD2019.ex23andMe.exUKBB",
      lava_ext
    )
  )

knitr::opts_chunk$set(echo = T, warning = F, message = F)
```

> Aim: preprocess GWASs for use with LAVA and LDSC

# Install dependencies
To run this .Rmd requires packages that are not available from CRAN/Bioconductor. These include `colochelpR` and `rutils`. If these are not already installed, run the code chunk below.

```{r install-dependencies, eval = F}

devtools::install_github("RHReynolds/colochelpR")
devtools::install_github("RHReynolds/rutils")
```

# Requirements for summary statistics {.tabset}

## LAVA

- Summary statistics must have the following columns:
    - **SNP / ID / SNPID_UKB/ SNPID / MarkerName / RSID / RSID_UKB**: SNP IDs
    - **A1 / ALT**: effect allele
    - **A2 / REF**: reference allele
    - **N / NMISS / OBS_CT / N_analyzed**: number of samples
    - **Z / T / STAT / Zscore**: if provided, no p-values or coefficients are needed; otherwise, please provide both:
    - **B / BETA / OR / logOdds**: effect size coefficients
    - **P**: p-values
- In addition, if using CHR:BP-based locations, we must ensure that all of the summary statistics have the same genome build (i.e. GRCh37). 

## LDSC 
- Provided the appropriate columns are available for LAVA, prepped files should be valid to use with LDSC.
- LDSC requires RS IDs in order to pre-process for LDSC, so ensure prepped files contain RS IDs.

# Pre-processing GWASs {.tabset}

- As both tools can use RS IDs and these are transferrable across genome builds (not to mention, many of the GWASs have already been prepped for LDSC using RS IDs), we will use RS IDs as our SNP column.
- We will ensure there are CHR and BP columns to indicate genomic coordinates, which we may wish to overlap with the [LAVA partitions](https://github.com/cadeleeuw/lava-partitioning)).
- Where to output these files?
    - **We will save these LAVA-formatted GWASs using the suffix `.lava.gz` in the commmon GWAS folder (for lab re-use).** This will also make it easier to find their file paths later, using the `list.files()` function.
    - **Remember to modify directory readmes to reflect addition of new files.**

## AD 2013
```{r AD2013-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "AD/AD_all_sum_stats/IGAP_stage_1.txt"
  )

# Read in first few rows to check data format
fread(file_path, nrows = 6)
```

- Most columns are present, but require renaming.
- N not defined for study, therefore will add column with N = 17,008 cases +	37,154 controls = `r c(17008	+ 37154) %>% format(scientific = F)`

```{r AD2013-preprocess, eval = F}

AD <-
  fread(file_path) %>%
  dplyr::mutate(
    P = as.numeric(Pvalue),
    N = 17008	+ 37154
  ) %>% 
  dplyr::select(
    CHR = Chromosome, 
    BP = Position, 
    SNP = MarkerName, 
    A1 = Effect_allele, 
    A2 = Non_Effect_allele, 
    BETA = Beta, 
    SE, 
    P,
    N
  )

fwrite(AD,
  file =
    stringr::str_c(
      gwas_dir,
      "AD/AD2013",
      lava_ext
    ),
  sep = "\t"
)
```

## AD 2019
```{r AD2019-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "AD2019/AD_sumstats_Jansenetal_2019sept.txt.gz"
  )

# Read in first few rows to check data format
fread(file_path, nrows = 6)
```

- All columns present. Just need to rename `Neff` to one of the accepted LAVA inputs.

```{r AD2019-preprocess, eval = F}

AD <-
  fread(file_path) %>%
  dplyr::select(CHR, BP, SNP, A1, A2, MAF = EAF, BETA, SE, Z, P, N = Neff)

fwrite(AD,
  file =
    stringr::str_c(
      gwas_dir,
      "AD2019/AD_sumstats_Jansenetal_2019sept",
      lava_ext
    ),
  sep = "\t"
)
```

## AD 2019, Kunkle et al.

```{r AD2019-kunkle-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "AD2019_Kunkle/Kunkle_etal_Stage1_results.txt.gz"
  )

# Read in first few rows to check data format
fread(file_path, nrows = 6)
```

- Most columns are present, but require renaming.
- N not defined for study, therefore will add column with N = 21,982 cases +	41,944 controls = `r c(21982 + 41944) %>% format(scientific = F)`

```{r AD2019-kunkle-preprocess, eval = F}

AD <-
  fread(file_path) %>%
  dplyr::mutate(
    N = 21982 + 41944,
    Pvalue = as.numeric(Pvalue)
  ) %>% 
  dplyr::select(
    CHR = Chromosome, 
    BP = Position, 
    SNP = MarkerName, 
    A1 = Effect_allele, 
    A2 = Non_Effect_allele, 
    BETA = Beta, 
    SE, 
    P = Pvalue,
    N
  )

fwrite(AD,
  file =
    stringr::str_c(
      gwas_dir,
      "AD2019_Kunkle/AD2019.Kunkle",
      lava_ext
    ),
  sep = "\t"
)
```

## AD 2021

```{r AD2021-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "AD2021/PGCALZ2sumstatsExcluding23andMe.txt.gz"
  )

# Read in first few rows to check data format
fread(file_path, nrows = 6)
```

- Most columns are present, but require renaming.
- Will also add a SNP column with RS ID. 
- Remove any SNPs with z-score of infinity.

```{r AD2021-preprocess, eval = F}

AD <-
  fread(file_path) %>%
  dplyr::select(
    CHR = chr, 
    BP = PosGRCh37, 
    A1 = testedAllele, 
    A2 = otherAllele, 
    Z = z, 
    P = p,
    N
  ) %>% 
  dplyr::mutate(CHR = as.factor(CHR)) %>%
  dplyr::filter(Z != "Inf") %>% 
  colochelpR::convert_loc_to_rs(
    df = .,
    dbsnp_144
  )

# Function colochelpR::convert_loc_to_rs does not remove CHR:BP associated with more than one rs id
# Nor does it remove CHR:BP locations that do not have associated rs ids (left as NA)
# Thus, will remove NAs and remove duplicated CHR:BP
AD <-
  AD %>%
  dplyr::mutate(CHR_BP = stringr::str_c(CHR, ":", BP)) %>%
  dplyr::group_by(CHR_BP) %>%
  dplyr::filter(!any(row_number() > 1))  %>%
  dplyr::ungroup() %>%
  dplyr::select(-CHR_BP) %>%
  tidyr::drop_na(SNP)

fwrite(AD,
  file =
    stringr::str_c(
      gwas_dir,
      "AD2021/AD2021",
      lava_ext
    ),
  sep = "\t"
)
```

## LBD
```{r LBD-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "LBD2020/LBD2020.txt"
  )

fread(file_path, nrows = 6)
```

- Already know that this GWAS is based on GRCh38. Therefore, will need liftover from GRCh38 to GRCh37. To do this, we will use `rutils::liftover_coord()`.
- Will also add a SNP column with RS ID. 
- Finally, let's remove some of the extra columns to limit the output file size.

```{r LBD-preprocess , eval = F}

LBD <-
  fread(file_path) %>%
  dplyr::select(CHR = CHROM, BP = POS, A1, A2, MAF = A1_FREQ, BETA, SE, Z = Z_STAT, P, N = OBS_CT) %>%
  dplyr::mutate(CHR = as.factor(CHR)) %>%
  rutils::liftover_coord(
    df = .,
    path_to_chain = "/data/liftover/hg38/hg38ToHg19.over.chain"
  ) %>%
  colochelpR::convert_loc_to_rs(
    df = .,
    dbsnp_144
  )

# Function colochelpR::convert_loc_to_rs does not remove CHR:BP associated with more than one rs id
# Nor does it remove CHR:BP locations that do not have associated rs ids (left as NA)
# Thus, will remove NAs and remove duplicated CHR:BP
LBD <-
  LBD %>%
  dplyr::mutate(CHR_BP = stringr::str_c(CHR, ":", BP)) %>%
  dplyr::group_by(CHR_BP) %>%
  dplyr::filter(!any(row_number() > 1))  %>%
  dplyr::ungroup() %>%
  dplyr::select(-CHR_BP) %>%
  tidyr::drop_na(SNP)

fwrite(LBD,
  stringr::str_c(
    gwas_dir,
    "LBD2020/LBD2020_hg19_rsids",
    lava_ext
  ),
  sep = "\t"
)
```

- It's worth noting that both processes may result in some loss of SNPs (see comparison of file line count below). This may result from:
    1. A location not being lifted over between builds.
    2. A location may not have an associated RS ID.

```{r LBD-check-n-lines}

n_original <- R.utils::countLines("/data/LDScore/GWAS/LBD2020/LBD2020.txt")
n_liftover <- R.utils::countLines("/data/LDScore/GWAS/LBD2020/LBD2020_hg19_rsids.lava.gz")

print(str_c("Number of lines in original file: ", n_original))
print(str_c("Number of lines in modified file: ", n_liftover))
```

## PD 2011

```{r PD2011-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "PD2011/PD-2011_Meta.txt"
  )

fread(file_path, nrows = 6)
```

- Liftover from GRCh36 to GRCh37 and add RS IDs.
- A1 and A2 are lowercase, so will change to upper case.
- N not defined for study, therefore will add column with N = 5,333 cases +	12,019 controls = `r c(5333 + 12019) %>% format(scientific = F)`

```{r PD2011-preprocess, eval = F}

PD <-
  fread(file_path) %>%
  dplyr::select(
    CHR, BP, 
    CHR_BP = MARKER,
    A1 = Allele1, 
    A2 = Allele2, 
    MAF = Freq1, 
    BETA = Effect, 
    SE = StdErr, 
    P = meta_P, 
    ) %>% 
  dplyr::mutate(
    CHR = as.factor(CHR),
    A1 = stringr::str_to_upper(A1),
    A2 = stringr::str_to_upper(A2),
    N = 5333 + 12019
  ) %>% 
  rutils::liftover_coord(
    df = .,
    path_to_chain = "/data/liftover/hg18/hg18ToHg19.over.chain"
  ) %>% 
  colochelpR::convert_loc_to_rs(
    df = .,
    dbsnp_144
  )

# Function colochelpR::convert_loc_to_rs does not remove CHR:BP associated with more than one rs id
# Nor does it remove CHR:BP locations that do not have associated rs ids (left as NA)
# Thus, will remove NAs and remove duplicated CHR:BP  
PD <-
  PD %>%
  dplyr::group_by(CHR_BP) %>%
  dplyr::filter(!any(row_number() > 1))  %>%
  dplyr::ungroup() %>%
  dplyr::select(-CHR_BP) %>%
  tidyr::drop_na(SNP)
  
fwrite(
  PD,
  stringr::str_c(
    gwas_dir,
    "PD2011/PD2011",
    lava_ext
  ),
  sep = "\t"
)
```

## PD 2019 (excluding 23andMe)
```{r PD2019-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "PD2019_meta5_ex23andMe/PD2019_ex23andMe_hg19.txt.gz"
  )

fread(file_path, nrows = 6)
```

- All columns are present, but a few need renaming.
- Number of samples currently split into cases and controls. These need to be summed to a total.

```{r PD2019-preprocess, eval = F}

PD <-
  fread(file_path) %>%
  dplyr::mutate(N = N_cases + N_controls) %>%
  dplyr::select(CHR, BP, SNP, A1, A2, MAF = freq, BETA = b, SE = se, P = p, N)

fwrite(
  PD,
  stringr::str_c(
    gwas_dir,
    "PD2019_meta5_ex23andMe/PD2019_ex23andMe_hg19",
    lava_ext
  ),
  sep = "\t"
)
```

## PD 2019 (excluding 23andMe and UK Biobank)
```{r PD2019-noUKBB-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "PD2019_meta5_ex23andMe_exUKBB/META_no_UKBB_no_231.tbl.gz"
  )

fread(file_path, nrows = 6)
```

- **Note:** this is not an official release of the PD summary statistics (supplied by Cornelis Blauwendraat, IPDGC), so use with care.
- Number of samples for each SNP added using the `Direction` column together with cohort sample numbers
    - The order of the 13 symbols (`+`, `-` or `?`) reflect the order of the cohorts meta-analysed, which is as follows:
        - Input File 1 : ../individual_cohort_sumstats_meta5/toMeta.dbgapNeuroX.tab
        - Input File 2 : ../individual_cohort_sumstats_meta5/toMeta.SHUL.tab
        - Input File 3 : ../individual_cohort_sumstats_meta5/toMeta.FINLAND_no_age.tab
        - Input File 4 : ../individual_cohort_sumstats_meta5/toMeta.HBS.tab
        - Input File 5 : ../individual_cohort_sumstats_meta5/toMeta.GILL_PD_C.tab
        - Input File 6 : ../individual_cohort_sumstats_meta5/toMeta.OSLO.tab
        - Input File 7 : ../individual_cohort_sumstats_meta5/toMeta.PDBP.tab
        - Input File 8 : ../individual_cohort_sumstats_meta5/toMeta.PPMI.tab
        - Input File 9 : ../individual_cohort_sumstats_meta5/toMeta.queensland.tab
        - Input File 10 : ../individual_cohort_sumstats_meta5/toMeta.SPAIN3.tab
        - Input File 11 : ../individual_cohort_sumstats_meta5/toMeta.TUBI_no_overlap.tab
        - Input File 12 : ../individual_cohort_sumstats_meta5/toMeta.VANCE.tab
        - Input File 13 : ../individual_cohort_sumstats_meta5/toMeta.COURAGE_UK.tab
    -  `+` or `-` indicate the variant direction of effect within a cohort; `?` indicate that that variant was not present in that dataset

```{bash PD2019-noUKBB-generate-n, eval = F}
# Have to navigate to root project folder for script to work (as it uses here package)
cd /home/rreynolds/misc_projects/neurodegen-psych-local-corr

nohup Rscript \
/home/rreynolds/misc_projects/neurodegen-psych-local-corr/scripts/00_generate_pd_n.R \
&>/home/rreynolds/misc_projects/neurodegen-psych-local-corr/logs/00_generate_pd_n.log&

```

- Filtering of summary statistics necessary and performed as recommended by Cornelis Blauweendraat:
    - Exclude MAF < 1%
    - Exclude `HetISq` > 80% (I^2 statistic measures heterogeneity on scale of 0-100%)
    - Exclude SNPs where SNP is not present in $\geq$ 2/3 of studies included. 
- **Note:** distribution of N across filtered SNPs is bimodally distributed (Figure \@ref(fig:PD2019-noUKBB-N)). This is problematic, as LDSC `munge_sumstats.py` removes any SNPs that have N < (90th percentile/1.5), unless another minimum is specified (in the filtered data, this accounts for `r ((PD %>% dplyr::filter(N < (quantile(PD$N, seq(0,1,0.1))["90%"])/1.5) %>% nrow()/nrow(PD)) * 100) %>% round(digits = 1)` of the data). 
    - Notably, the default specified by the code does not reflect the default behaviour described in the arguments (see issue [#335](https://github.com/bulik/ldsc/issues/335)). In the arguments descriptions, the default behaviour should be N < (90th percentile/2). 
    - Given the bimodal distribution, and the peak around ~ 13,000, chose to use `--n-min 13655` N < median(N). This equates to `r median(PD$N)`, as compared to `r quantile(PD$N, seq(0,1,0.1)["90%"])/1.5` (N < (90th percentile/1.5)) or `r quantile(PD$N, seq(0,1,0.1))["90%"]/2` (N < (90th percentile/2)), and accounts for `r ((PD %>% dplyr::filter(N < median(N)) %>% nrow()/nrow(PD)) * 100) %>% round(digits = 1)` of the data.
- Will also add a SNP column with RS ID. 

```{r PD2019-noUKBB-N, echo = F, fig.cap = "Histogram of SNP N."}

PD %>% 
  ggplot(
    aes(
      x = N
    )
  ) + 
  geom_histogram(binwidth = 500)

```


```{r PD2019-noUKBB-preprocess, eval = F}

# Filtering by MAF, I^2 and number of cohorts where variant is present
PD_filtered <-
  fread(file_path) %>% 
  dplyr::mutate(
    n_cohort_present = 
      stringr::str_count(Direction, c("\\+|-")),
  ) %>% 
  dplyr::filter(
    Freq1 >= 1/100,
    HetISq <= 80,
    n_cohort_present >= (2/3 * 13)
  )

# Tidy and rename columns for use with LAVA
# Add RS ID
PD <- 
  PD_filtered %>% 
  dplyr::select(
    CHR_BP = MarkerName,
    A1 = Allele1, 
    A2 = Allele2, 
    MAF = Freq1, 
    BETA = Effect, 
    SE = StdErr, 
    P = `P-value`
    ) %>% 
  dplyr::inner_join(
    PD_N %>% 
      dplyr::select(
        CHR_BP = MarkerName,
        N
        )
  ) %>% 
  tidyr::separate(
    col = CHR_BP,
    sep = ":",
    into = c("CHR", "BP"), 
    remove = FALSE
  ) %>% 
  dplyr::mutate(
    CHR = as.factor(CHR),
    A1 = stringr::str_to_upper(A1),
    A2 = stringr::str_to_upper(A2),
  ) %>% 
  colochelpR::convert_loc_to_rs(
    df = .,
    dbsnp_144
  )

# Function colochelpR::convert_loc_to_rs does not remove CHR:BP associated with more than one rs id
# Nor does it remove CHR:BP locations that do not have associated rs ids (left as NA)
# Thus, will remove NAs and remove duplicated CHR:BP  
PD <-
  PD %>%
  dplyr::group_by(CHR_BP) %>%
  dplyr::filter(!any(row_number() > 1))  %>%
  dplyr::ungroup() %>%
  dplyr::select(-CHR_BP) %>%
  tidyr::drop_na(SNP)

fwrite(
  PD,
  stringr::str_c(
    gwas_dir,
    "PD2019_meta5_ex23andMe_exUKBB/PD2019.ex23andMe.exUKBB",
    lava_ext
  ),
  sep = "\t"
)
```

## BIP
```{r BIP-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "BIP2021/pgc-bip2021-all.vcf.tsv.gz"
  )

# First 72 lines are written text, therefore must remove these to format summary statistics
fread(file_path, skip = 72, nrow = 6)
```

- All columns are present, but a few need renaming.
- Frequency of A1 not provided across all cohorts, but instead across cases and controls. Therefore, averaged the FCAS and FCON to get MAF.
- Number of samples currently split into cases and controls. These need to be summed to a total.

```{r BIP-preprocess, eval = F}

BIP <-
  fread(file_path, skip = 72) %>%
  dplyr::mutate(
    MAF = (FCAS + FCON)/2,
    N = NCAS + NCON
    ) %>%
  dplyr::select(
    CHR = `#CHROM`, 
    BP = POS, 
    SNP = ID, 
    A1, A2, MAF, BETA, SE, 
    P = PVAL, 
    N
    )

fwrite(
  BIP,
  stringr::str_c(
    gwas_dir,
    "BIP2021/BIP2021",
    lava_ext
  ),
  sep = "\t"
)
```

## MDD
```{r MDD-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "MDD2019_ex23andMe/PGC_UKB_depression_genome-wide.txt.gz"
  )

fread(file_path, nrow = 6)
```

- Missing chromosome and base pair positions for SNPs, 
- A1 and A2 are lowercase, so will change to upper case.
- N not defined for study, therefore will add column with N = 170,756 cases +	329,443 controls = `r c(170756 + 329443) %>% format(scientific = F)`

```{r MDD-preprocess, eval = F}

MDD <-
  fread(file_path) %>%
  dplyr::mutate(
    A1 = stringr::str_to_upper(A1),
    A2 = stringr::str_to_upper(A2),
    N = 170756 + 329443
    ) %>%
  colochelpR::convert_rs_to_loc(
    SNP_column = "MarkerName", 
    dbSNP = dbsnp_144
    ) %>% 
  tidyr::separate(
    col = "loc",
    into = c("CHR", "BP")
  ) %>% 
  dplyr::select(
    CHR, BP, 
    SNP = MarkerName, 
    A1, A2, 
    MAF = Freq, 
    logOdds = LogOR, 
    SE = StdErrLogOR, 
    P, 
    N
    )

fwrite(
  MDD,
  stringr::str_c(
    gwas_dir,
    "MDD2019_ex23andMe/MDD2019",
    lava_ext
  ),
  sep = "\t"
)
```

## SCZ
```{r SCZ-check-cols}

file_path <-
  file.path(
    gwas_dir,
    "SCZ2018/RS_clozuk_pgc2.meta.sumstats.txt"
  )

fread(file_path, nrow = 6)
```

- - N not defined for study, therefore will add column with N = 40,675 cases + 64,643 controls = `r c(40675 + 64643) %>% format(scientific = F)`


```{r SCZ-preprocess, eval = F}

SCZ <-
  fread(file_path) %>% 
  dplyr::mutate(
    N = 40675 + 64643
  ) %>% 
  dplyr::select(
    CHR, BP, SNP, A1, A2, 
    MAF = Freq.A1, 
    OR, SE, P, N
    )

fwrite(
  SCZ,
  stringr::str_c(
    gwas_dir,
    "SCZ2018/SCZ2018",
    lava_ext
  ),
  sep = "\t"
)
```

# Listing available LAVA files
```{r list-lava-files}

list.files(
  path = gwas_dir,
  full.names = F,
  recursive = T,
  pattern = lava_ext
)
```

# Session info
```{r session-info}
# Session info
library("sessioninfo")

options(width = 120)

session_info()
```
